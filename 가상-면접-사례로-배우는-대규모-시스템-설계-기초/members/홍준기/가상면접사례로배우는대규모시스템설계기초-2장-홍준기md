## 압축

책의 내용:
* 단순한 압축 알고리즘은 빠르다.
	* 그러므로 데이터를 인터넷으로 전송하기 전에 가능하면 압축하라.
	* json 응답에 대한 압축 알고리즘은 매우 빠르다곤 한다.


### 사례

> 응답 크기가 큰 특정 몇 API 는 사용자가 최초 접속 시 반드시 호출 되어야만 하고, 데이터 특성상 자주 변경되어 HTTP 의 [Cache-Control](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control) 을 사용해 캐싱하기 어려웠다. [ETag](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag) 를 적용하는 것도 고려했지만, 데이터가 너무 크고 빈번하게 변동되어 매번 식별자를 생성하는 작업이 비효율적이라 판단했다. 대신, [Last-Modified](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Last-Modified) 를 적용해서 일부 문제를 해소해 보았다.
> 
> 하지만, 문제는 데이터가 자주 바뀌어서 캐시무효화가 자주 발생한다는 점이었다. 또한, 캐시가 없는 상태에서의 요청은 여전히 부담이 될 수 밖에 없었다. API 구조상 응답 자체를 분리하거나 Lazy 로딩으로 변경 하기도 어려워, 고민하다가 압축을 선택하게 되었다.
> 
> 출처 : https://jongmin4943.tistory.com/entry/Spring-%ED%8A%B9%EC%A0%95-API%EC%9D%98-Response-%EC%95%95%EC%B6%95%ED%95%98%EA%B8%B0

### 사례 분석

* 상황 : 응답 크기가 커서 매번 다 다운로드하면 느리고 트래픽 비용이 걱정됨. -> 캐싱 필요
* Cache-Control : max-age=3600과 같은 방식
	* 당연히 쓰기 어려움
* Cache-Control : no-cache + ETag
	* ETag는 응답 해싱을 통해 빠른 정합성 비교 방법
		* 해싱 비용이 낮아야 의미있는 대처법이 됨.
	* 응답 크기가 커서 서버에서 해싱하는 데 부담
* Cache-Control : no-cache + Last-Modified
	* ETag 처럼 정교한 정합성 비교를 포기, 데이터의 수정 시간으로만 정합성 비교

하지만, 결국 데이터가 빈번하게 변동되어 캐시에 대한 생각 자체가 의미가 크지 않게 됨.

압축에는 **압축의 정도**를 조절할 수 있는 압축 레벨을 설정할 수 있는데, 해당 블로그 필자는 엔드포인트에 적절한 압축레벨을 조절할 수 있는 압축 AOP를 구축하고 어노테이션 기반으로 설정해서 해결한 듯 하다.

* 개선 전 : 147kb - 48ms
* 개선 후 : 7.7kb - 72ms

해당 블로그의 결과가 이상하다. 

CPU 사용량의 문제가 아니라면 압축은 속도가 빨라져야한다. 데이터 크기와 속도는 비례하기 때문이다.

아마 147kb나 7.7kb나 네트워크 기준에서는 별 차이 없는 정도의 크기인 것이다. 느려진 이유는 아마 서버에서 이 압축을 처리하기 위해 든 알고리즘적 시간일 것이다.

또 하나 궁금증이 생기는 것은 **왜 대역폭을 이 사람은 의식한 것일까?** 라는 것이다.

대역폭은 도로의 넓이일 뿐, 차가 많아도 막히지만 가긴 가는건데 말이다.

AWS 같은 클라우드 환경에서는 **"대역폭 초과"가 단순한 '지연'을 넘어 '재앙(장애)'이 되는 이유**가 된다.

* 예: `t3.medium` 인스턴스는 평소에 **"기준 대역폭(Baseline)"**이 정해져 있고, 그 이상을 쓰려면 **"크레딧"**을 소모해야 함.
* 크레딧이 다 떨어지면? AWS가 강제로 속도를 **확 깎아버린린다(Throttling).**

### gzip 압축 레벨
#### 단계별 특징 (보통 1~9단계)

- **레벨 1 (Fastest):**
        
    - **장점:** CPU를 제일 적게 먹고, 속도가 엄청 빠름.
        
    - **압축률:** 생각보다 좋습니다. (JSON 기준 50~60%는 줄어듦)
        
- **레벨 6 (Default):**
    
    - **특징:** 표준 설정임
        
    - **장점:** CPU 사용량과 압축률 사이의 **황금 밸런스(Sweet Spot)**
        
- **레벨 9 (Best Compression):**
        
    - **단점:** **CPU가 폭발**할 수 있습니다. 처리 속도가 느려짐.
        
    - **압축률:** 레벨 6이랑 비교했을 때 **별 차이 안 남.**

# 압축의 기준

## 기준 1. 데이터 크기: 최소 2KB 이상인가?

* 데이터가 너무 작으면 배보다 배꼽이 더 커짐.

* 1kb를 압축해서 700byte로 줄여봤자 TCP 패킷 하나에 담김.

* Spring Boot의 기본 설정(`server.compression.min-response-size`)도 **2048 (2KB)**.

> **실무 TIP** 
> 응답 JSON이 `{ "result": "ok" }` 처럼 단순한 API라면 압축 NONO
반면, 리스트 조회나 상세 정보 조회라면 보통 2KB는 가뿐히 넘으니 하는게 좋음.

## 기준 2. 트래픽 빈도 (RPS): "CPU가 쉴 틈이 있는가?"

"데이터가 자주 변경된다"는 건, 매 요청마다 **DB 조회 + JSON 변환 + 압축 연산**을 풀코스로 밟아야 한다는 뜻

`초당 요청 수(RPS)` 가 매우 많은 상황(우리의 서버 CPU가 한계치에 다다른 상황)에서는 압축이 추가될 경우 CPU가 뻗어버림.

### 기준 3. 우리는 속도가 중요하고 돈이 많은가?

돈이 많다면 서버를 스케일 아웃하는데에 문제가 없을 것이다.

우리가 느끼는 전체 응답 시간은 `지연시간 + 데이터 크기 / 대역폭` 이다.

데이터 크기가 크다는 것은 여러 패킷으로 쪼개져 보내진다는 것을 의미한다. 그만큼 느려진다.

# 압축은 어디에서?

스프링 부트(Spring Boot)의 기본 설정 정책은 다음과 같다.

1. **압축 기능 자체:** **기본적으로 꺼져 있다 (Disabled).**
    
    - 개발자가 명시적으로 켜지 않으면, 2KB가 넘든 100MB가 넘든 압축을 **전혀 안한다.**
        
2. **압축 기준 용량 (Threshold):** **기본값이 2KB이다.**
    
    - 만약 개발자가 압축 기능을 **"켜면(Enabled)"**, 그때부터는 별도 설정을 안 해도 **"2KB 이상인 것만 압축"**하도록 세팅되어 있다.

```YAML
server:
  compression:
    enabled: false        # <--- 여기가 핵심! 기본은 꺼져 있음 (False)
    min-response-size: 2048 # <--- 기능이 켜지면, 기본 기준은 2KB (2048 Byte)
    mime-types: text/html, text/xml, text/plain, text/css, text/javascript, application/javascript, application/json
```

* **하지만 보통적으로는 API Gateway, CDN이 존재한다면 서버 앞단에서 수행.**

